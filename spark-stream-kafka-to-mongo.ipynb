{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-20T20:13:46.023990Z",
     "iopub.status.busy": "2025-02-20T20:13:46.023677Z",
     "iopub.status.idle": "2025-02-20T20:14:21.781759Z",
     "shell.execute_reply": "2025-02-20T20:14:21.780766Z",
     "shell.execute_reply.started": "2025-02-20T20:13:46.023963Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install pyspark == 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T20:14:25.973914Z",
     "iopub.status.busy": "2025-02-20T20:14:25.973583Z",
     "iopub.status.idle": "2025-02-20T20:14:28.769282Z",
     "shell.execute_reply": "2025-02-20T20:14:28.768488Z",
     "shell.execute_reply.started": "2025-02-20T20:14:25.973858Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.5.2\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: py4j\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T20:22:50.014299Z",
     "iopub.status.busy": "2025-02-20T20:22:50.013959Z",
     "iopub.status.idle": "2025-02-20T20:51:14.017085Z",
     "shell.execute_reply": "2025-02-20T20:51:14.015844Z",
     "shell.execute_reply.started": "2025-02-20T20:22:50.014267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf \n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType # For defining the schema of our Kafka messages\n",
    "from transformers import pipeline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s') # Configure logging to only display errors\n",
    "\n",
    "\n",
    "# Create a checkpoint dir where Spark will store its progress\n",
    "checkpoint_dir = \"/kaggle/working/checkpoints/kafka_to_mongo\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Create dict which holds our Kafka and MongoDB creds\n",
    "config = {\n",
    "    \"kafka\": {\n",
    "    'bootstrap.servers': '',\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanisms':'PLAIN',\n",
    "    'sasl.username':'',\n",
    "    'sasl.password':'',\n",
    "    'client.id':'json-serial-producer'\n",
    "},\n",
    "    \"mongodb\": {\n",
    "        \"uri\": 'mongodb+srv://<username>:<password>@yelp-cluster.tp3ga.mongodb.net/?retryWrites=true&w=majority',\n",
    "        \"database\": 'reviewsdb',\n",
    "        \"collection\": \"enriched_reviews_collection\"\n",
    "    }\n",
    "}\n",
    "\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model = \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# analyze_sentiment takes text as input and return the sentiment label (Positive/Negative)\n",
    "def analyze_sentiment(text):\n",
    "    if text and isinstance(text, str):\n",
    "        try:\n",
    "            result = sentiment_pipeline(text)[0]\n",
    "            return result['label']\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in sentiment analysis: {e}\")\n",
    "            return \"Error\"\n",
    "    return \"Empty or Invalid\"\n",
    "\n",
    "# Wrap analyze_sentiment in UDF which allows Spark to appy it to the streaming data\n",
    "sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "def read_from_kafka_and_write_to_mongo(spark):\n",
    "    topic = \"raw_topic\"\n",
    "\n",
    "    # Define the schema of the Kafka messages\n",
    "    schema = StructType([\n",
    "        StructField(\"review_id\",StringType()),\n",
    "        StructField(\"user_id\",StringType()),\n",
    "        StructField(\"business_id\",StringType()),\n",
    "        StructField(\"stars\",FloatType()),\n",
    "        StructField(\"useful\",IntegerType()),\n",
    "        StructField(\"funny\",IntegerType()),\n",
    "        StructField(\"cool\",IntegerType()),\n",
    "        StructField(\"text\",StringType()),\n",
    "        StructField(\"date\",StringType())\n",
    "    ])\n",
    "    \n",
    "    stream_df = (spark.readStream\n",
    "                 .format(\"kafka\")\n",
    "                 .option(\"kafka.bootstrap.servers\",config['kafka']['bootstrap.servers'])\n",
    "                 .option(\"subscribe\",topic) # Subscribe to the raw_topic\n",
    "                 .option(\"kafka.security.protocol\", config['kafka']['security.protocol'])\n",
    "                 .option(\"kafka.sasl.mechanism\",config['kafka']['sasl.mechanisms'])\n",
    "                 .option(\"kafka.sasl.jaas.config\",\n",
    "                        f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{config[\"kafka\"][\"sasl.username\"]}\" '\n",
    "                        f'password=\"{config[\"kafka\"][\"sasl.password\"]}\";')\n",
    "                 .option(\"failOnDataLoss\",\"false\")\n",
    "                 .load()\n",
    "                )\n",
    "    \n",
    "    # Convert the raw Kafka messages into structured data frame using from json\n",
    "    parsed_df = stream_df.select(from_json(col('value').cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "    \n",
    "    # Enrich the parsed data frame by adding a new column called 'sentiment'\n",
    "    enriched_df = parsed_df.withColumn(\"sentiment\", sentiment_udf(col('text')))\n",
    "    \n",
    "    # Write the enriched data frame to MongoDB\n",
    "    query = (enriched_df.writeStream\n",
    "             .format(\"mongodb\")\n",
    "             .option(\"spark.mongodb.connection.uri\", config['mongodb']['uri'])\n",
    "             .option(\"spark.mongodb.database\", config['mongodb']['database'])\n",
    "             .option(\"spark.mongodb.collection\", config['mongodb']['collection'])\n",
    "             .option(\"checkpointLocation\", checkpoint_dir)\n",
    "             .outputMode(\"append\")\n",
    "             .start()\n",
    "             .awaitTermination()\n",
    "            )\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    spark = (SparkSession.builder\n",
    "          .appName(\"KafkaStreamToMongo\")\n",
    "          .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2,org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\")\n",
    "          .getOrCreate()\n",
    "          )\n",
    "    read_from_kafka_and_write_to_mongo(spark)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
