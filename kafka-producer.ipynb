{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-20T20:17:46.982784Z",
     "iopub.status.busy": "2025-02-20T20:17:46.982454Z",
     "iopub.status.idle": "2025-02-20T20:17:54.597344Z",
     "shell.execute_reply": "2025-02-20T20:17:54.595832Z",
     "shell.execute_reply.started": "2025-02-20T20:17:46.982763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T18:30:58.123351Z",
     "iopub.status.busy": "2025-02-20T18:30:58.122994Z",
     "iopub.status.idle": "2025-02-20T18:31:01.838421Z",
     "shell.execute_reply": "2025-02-20T18:31:01.837018Z",
     "shell.execute_reply.started": "2025-02-20T18:30:58.123322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: confluent-kafka\n",
      "Version: 2.8.0\n",
      "Summary: Confluent's Python client for Apache Kafka\n",
      "Home-page: https://github.com/confluentinc/confluent-kafka-python\n",
      "Author: \n",
      "Author-email: \"Confluent Inc.\" <support@confluent.io>\n",
      "License: \n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-20T20:28:22.699343Z",
     "iopub.status.busy": "2025-02-20T20:28:22.699007Z",
     "iopub.status.idle": "2025-02-20T20:50:57.006206Z",
     "shell.execute_reply": "2025-02-20T20:50:57.003312Z",
     "shell.execute_reply.started": "2025-02-20T20:28:22.699321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "# Setup kafka producer config\n",
    "conf = {\n",
    "    'bootstrap.servers': '',\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanisms':'PLAIN',\n",
    "    'sasl.username':'Z',\n",
    "    'sasl.password':'',\n",
    "    'client.id':'json-serial-producer'\n",
    "}\n",
    "\n",
    "producer = Producer(conf) # Create an instance of the Producer class\n",
    "\n",
    "# Topic name\n",
    "topic = \"raw_topic\"\n",
    "\n",
    "# Delivery report callback\n",
    "def delivery_report(err,msg):\n",
    "    if err:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Message delivered successfully! Key: {msg.key()}\")\n",
    "\n",
    "# Read checkpoint        \n",
    "def read_checkpoint(checkpoint_file):\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as file:\n",
    "            return int(file.read().strip())\n",
    "    return 0\n",
    "\n",
    "# Write checkpoint\n",
    "def write_checkpoint(checkpoint_file, index):\n",
    "    with open(checkpoint_file, 'w') as file:\n",
    "        file.write(str(index))\n",
    "    print(f\"Checkpoint updated to line: {index}\")\n",
    "\n",
    "# Handle dates within our json records    \n",
    "def handle_date(obj):\n",
    "    if isinstance(obj, pd.Timestamp):\n",
    "        return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n",
    "    \n",
    "# Stream JSON serially\n",
    "def stream_json_serially(file_path, checkpoint_file='/kaggle/working/checkpoint.txt'):\n",
    "    last_sent_index = read_checkpoint(checkpoint_file)\n",
    "    \n",
    "    with open(file_path,'r') as file:\n",
    "        for idx, line in enumerate(file):\n",
    "            if idx < last_sent_index: # Check if the current index is less than the last sent\n",
    "                continue\n",
    "                \n",
    "            try: \n",
    "                record = json.loads(line) # Attempt to decode line from json \n",
    "                producer.produce( # Call the prodece method\n",
    "                    topic,\n",
    "                    key=str(record['review_id']),\n",
    "                    value=json.dumps(record, default=handle_date).encode('utf-8'), # Applying our handle_date func to ensure any dates are formatted correctly\n",
    "                    callback=delivery_report\n",
    "                )\n",
    "                \n",
    "                producer.flush() # Ensure that the message is sent to Kafka immediatly\n",
    "                \n",
    "                write_checkpoint(checkpoint_file, idx + 1) # Update our checkpoint file with  the new index\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Failed to decode JSON: {e}\")\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    stream_json_serially('/kaggle/input/yelp-dataset/yelp_academic_dataset_review.json')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 10100,
     "sourceId": 3316532,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
